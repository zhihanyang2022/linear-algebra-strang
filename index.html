<html>

<style>
.content {
  max-width: 1000px;
  margin: auto;
  text-align: center;
}
</style>
<body>

<div class="content">

<p><strong>Below are my notes for <u>MIT 18.S096 Matrix Calculus For Machine Learning And Beyond</u>:</strong></p>

<i>Lecture 4 Part 1: Gradients and Inner Products in Other Vector Space</i> [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">PDF</a>]</li>
<ul>
<li>Riesz representation theorem</li>
<li>Gradient of Frobenius norm of A with respect to A</li>
<li>Gradient of x.T @ A @ y with respect to A</li>
<li>Gradient of sum(A) with respect to A</li>
</ul>
	
<i>Lecture 5 Part 1: Derivative of Matrix Determinant and Inverse</i> [<a href="./matrix_calculus/5_1_derivative_of_matrix_determinant_and_inverse.pdf">PDF</a>]</li>
<ul>
<li>Norms and derivatives</li>
<li>Derivative of matrix determinant -- Jacobi's formula (d(det(A)) = ...)</li>
<li>Direct proof using Laplace expansion</li>
<li>Fancy proof using the similar behavior of determinant and trace near identity</li>
<li>Applications (Newton's method for finding eigenvalues, log determinant)</li>
<li>Derivative of matrix inverse</li>
</ul>

<i>Lecture 5 Part 2: Forward-mode Automatic Differentiation using Dual Numbers</i> [<a href="./matrix_calculus/5_2_forward_mode_automatic_differentiation_using_dual_numbers.ipynb">ipynb</a>]</li>
<ul>
</ul>

<i>Lecture 5 Part 3: Reverse-mode Automatic Differentiation</i> [<a href="./matrix_calculus/5_2_forward_mode_automatic_differentiation_using_dual_numbers.ipynb">ipynb</a>]</li>


</div>

</html>