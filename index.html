<html>

<!-- 
<style>
.content {
  max-width: 1000px;
  margin: auto;
  text-align: center;
}
.box {
  width: 1000px;
  padding: 10px;
  margin: auto;
  text-align: center;
}

</style>
 -->

<p><strong>Zhihan's notes for <u>MIT 18.S096 Matrix Calculus For Machine Learning And Beyond</u></strong></p>
<img src="./media/death-valley-road.jpg" width=300px>

<p>To view a Jupyter notebook (ipynb), copy the link and paste it here at <a href="https://nbviewer.org/">https://nbviewer.org/</a>.</p>

<p style="color: red">Lecture 4 Part 1/2: Gradients and Inner Products in Other Vector Space [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">PDF</a>]</p>
<ul>
<li>Riesz representation theorem</li>
<li>Gradient of Frobenius norm of A with respect to A</li>
<li>Gradient of x.T @ A @ y with respect to A</li>
<li>Gradient of sum(A) with respect to A</li>
</ul>
	
<p style="color: red">Lecture 5 Part 1/3: Derivative of Matrix Determinant and Inverse [<a href="./matrix_calculus/5_1_derivative_of_matrix_determinant_and_inverse.pdf">PDF</a>]</p>
<ul>
<li>Norms and derivatives</li>
<li>Derivative of matrix determinant -- Jacobi's formula (d(det(A)) = ...)</li>
<li>Direct proof using Laplace expansion</li>
<li>Fancy proof using the similar behavior of determinant and trace near identity</li>
<li>Applications (Newton's method for finding eigenvalues, log determinant)</li>
<li>Derivative of matrix inverse</li>
</ul>

<p style="color: red">Lecture 5 Part 2/3: Forward-mode Automatic Differentiation using Dual Numbers [<a href="./matrix_calculus/5_2_forward_mode_automatic_differentiation_using_dual_numbers.ipynb">ipynb</a>]</p>
<ul>
</ul>

<p style="color: red">Lecture 5 Part 3/3: Reverse-mode Automatic Differentiation [<a href="./matrix_calculus/5_3_reverse_mode_automatic_differentiation.ipynb">ipynb</a>]</p>
<ul>
</ul>

<p style="color: red">Lecture 6 Part 1/2: Adjoint diff for ODE (skipped for now; need ODE review)</p>
<ul>
</ul>

<p style="color: red">Lecture 6 Part 2/2: Calculus of Variations and Gradients of Functionals [<a href="./matrix_calculus/6_2_calculus_of_variations_and_gradients_of_functionals.pdf">PDF</a>]</p>
<ul>
<li>Calculus of variations - the basic idea</li>
<li>A natural definition of dot product and norm of functions</li>
<li>Example: differentiating f(u)=\int_0^1 sin(u(x)) dx w.r.t. u</li>
<li>Example: differentiating the arc length formula + finding minimum using the gradient</li>
<li>Generalization of the 2nd example into a 2nd-order ODE (Euler-Lagrange equation)</li>
</ul>

</html>