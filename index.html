<style>
li {
  margin: 10px 0;
}
html {
  font-family: Georgia, serif;
  margin: 50 50; 
  background-color: rgb(255, 255, 249);
  font-size: 15px;
}
</style>

<html>

<!-- 
<style>
.content {
  max-width: 1000px;
  margin: auto;
  text-align: center;
}
.box {
  width: 1000px;
  padding: 10px;
  margin: auto;
  text-align: center;
}

</style>
 -->

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
 
<title>Matrix Calculus Notes</title>

<h2>MIT 18.S096 Matrix Calculus For Machine Learning And Beyond</h2>

<p>Notes written by Zhihan during March-April 2024 in Suzhou, Jiangsu, China</p>

<p>Awesome additional resources:</p>

<ul>
<li><b>Matrix Differential Calculus with Applications in Statistics and Econometrics.</b> This is a textbook written by Jan R. Magnus and Heinz Neudecker and published in 1988. Check out Section 5.3 in which they defined the differential for scalar-to-scalar functions.</li>
<li><b>Old and New Matrix Algebra Useful for Statistics.</b> This is a collection of notes on Magnus and Neudecker (1988) written by Thomas P. Minka.</li>
<li><b>Machine Learning & Simulation.</b> This is a YouTube channel on advanced machine learning created by Felix Matteo Kohler (the "o" with double dots). Check out his videos on autodiff pushforward / pullback rules.</li>
<li><b>Numerical optimization.</b> This is a textbook written by Jorge Nocedal and Stephen J. Wright and published in 1999. Check out Chapter 8 in which they discussed finite-difference approximation and automatic differentiation.</li>
</ul>

<p>To view a Jupyter notebook (ipynb), copy its link and paste it here at <a href="https://nbviewer.org/">https://nbviewer.org/</a>.</p>

<hr>

<p style="color: gray; font-weight: bold">Lecture 1 Part 1/2: Introduction and Motivation (not much except this table)</p>

<img src="./matrix_calculus/1_1_introduction_and_motivation.png" width=500px>

<p>Comments:</p>

<ul>
<li>Gradients of functions with matrix input and scalar input are sometimes easier to derive elementwise.</li>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Lecture 1 Part 2/2: Derivatives as Linear Operators [<a href="./matrix_calculus/1_2_gradient_as_linear_operator.pdf">PDF</a>]</p>

<p>Content:</p>

<ul>
<li>Differential notation for scalar-to-scalar functions and vector-to-scalar functions</li>
<li>Gradient of \( \mathbf{x}^T A \mathbf{x} \) wrt \( \mathbf{x} \)</li>
</ul>

<p>Comments:</p>

<ul>
<li>So far vector-to-vector and matrix-to-matrix functions are not mentioned.</li>
<li>I don't like the course's treatment of df and dx as "infinitesimal changes". I prefer the treatment by Magnus and Neudecker's Matrix Differential Calculus
in which they define df as the part of f(x+dx) - f(x) that's linear in dx - nothing needs to be infinitesimal; I also follow this treatment in my notes. The two treatments are somewhat equivalent, but I found the second one better for pedagogy.
</ul>

<hr>

<p style="color: red; font-weight: bold"">Lecture 2 Part 1/2: Derivatives in Higher Dimensions: Jacobians and Matrix Functions [<a href="./matrix_calculus/2_1_derivatives_in_higher_dimensions_jacobians_and_matrix_functions.pdf">PDF</a>]</p>

<p>Content:</p>

<ul>
<li>Definition of differentiability, derivative and differential for vector-to-vector functions</li>
<li>Gradient of \( \mathbf{x} \odot \mathbf{x} \) wrt \( \mathbf{x} \)
<li>Gradient of \( \mathbf{x} \mathbf{x}^T \mathbf{x} \) wrt \( \mathbf{x} \)
<li>Chain rule: derivative of a composition of vector-to-vector functions</li>
<li>Cauchy's rule of invariance</li>
</ul>

<p>Comments:</p>

<ul>
<li>They didn't actually start on matrix functions until the next lecture.</li>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Lecture 2 Part 2/2: Vectorization of Matrix Functions + Lecture 3 Part 1/2: Kronecker Products and Jacobians [<a href="./matrix_calculus/2_2_and_3_1_matrix_jacobians_via_vectorization.pdf">PDF</a>]</p>

<p>Content (putting two lectures together because they are so relevant to each other):</p>

<ul>
<li>Differential notation for matrix-to-matrix functions</li>
<li>Gradient of \( A^2 \) wrt \( A \)</li>
<li>Gradient of \( A^3 \) wrt \( A \)</li>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Lecture 3 Part 2/2: Finite-Difference Approximations [<a href="./matrix_calculus/3_2_finite_difference_approximations.pdf">PDF</a>]</p>

<p>Content:</p>

<ul>
<li>Finite-difference approximation for vector-to-scalar functions</li>
<li>Finite-difference approximation for matrix-to-matrix functions</li>
</ul>

<p>Comments:</p>

<ul>
<li>I find this lecture a bit hand-wavy, so for the first part of my notes I followed Nocedal and Wright's Numerical Optimization.</li>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Problem Set 1 [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">Handout</a>] [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">My solution</a>]</p>

<hr>

<p style="color: red; font-weight: bold"">Lecture 4 Part 1/2: Gradients and Inner Products in Other Vector Space [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">PDF</a>]</p>

<p>Content:</p>

<ul>
<li>Riesz representation theorem</li>
<li>Gradient of \( ||A||_F \) wrt A</li>
<li>Gradient of \( \mathbf{x}^T A \mathbf{y} \) wrt \(A\)</li>
<li>Gradient of \( \sum_{ij} A_{ij} \) wrt \(A\)</li>
</ul>
	
<hr>
	
<p style="color: red; font-weight: bold"">Lecture 4 Part 2/2: Nonlinear Root Finding, Optimization, and Adjoint Gradient Methods [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">PDF</a>]</p>
	
<hr>
	
<p style="color: red; font-weight: bold"">Lecture 5 Part 1/3: Derivative of Matrix Determinant and Inverse [<a href="./matrix_calculus/5_1_derivative_of_matrix_determinant_and_inverse.pdf">PDF</a>]</p>

<p>Content:</p>

<ul>
<li>Norms and derivatives</li>
<li>Derivative of matrix determinant -- Jacobi's formula \( d(\text{det}(A)) = \ldots \)</li>
<li>Direct proof using Laplace expansion</li>
<li>Fancy proof using the similar behavior of determinant and trace near identity</li>
<li>Applications (Newton's method for finding eigenvalues, log determinant)</li>
<li>Derivative of matrix inverse</li>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Lecture 5 Part 2/3: Forward-mode Automatic Differentiation using Dual Numbers [<a href="./matrix_calculus/5_2_forward_mode_automatic_differentiation_using_dual_numbers.ipynb">ipynb</a>]</p>
<ul>
</ul>

<p>Pushforward / JVP rules [PDF]</p>

<hr>

<p style="color: red; font-weight: bold"">Lecture 5 Part 3/3: Differentiation on Computational Graphs [<a href="./matrix_calculus/5_3_reverse_mode_automatic_differentiation.ipynb">ipynb</a>]</p>
<ul>
</ul>

<p>Pullback / VJP rules [PDF]</p>

<hr>

<p style="color: gray; font-weight: bold"">Lecture 6 Part 1/2: Adjoint differentiation of ODE solutions (skipped for now; need ODE review)</p>
<ul>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Lecture 6 Part 2/2: Calculus of Variations and Gradients of Functionals [<a href="./matrix_calculus/6_2_calculus_of_variations_and_gradients_of_functionals.pdf">PDF</a>]</p>

<p>Content:</p>

<ul>
<li>Calculus of variations - the basic idea</li>
<li>A natural definition of dot product and norm of functions</li>
<li>Example: differentiating \( f(u)=\int_0^1 sin(u(x)) dx \) wrt \( u \)</li>
<li>Example: differentiating the arc length formula + finding minimum using the gradient</li>
<li>Generalization of the 2nd example into a 2nd-order ODE (Euler-Lagrange equation)</li>
</ul>

<hr>

<p style="color: red; font-weight: bold"">Problem Set 2 [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">Handout</a>] [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">My solution</a>]</p>

<hr>

<p style="color: gray; font-weight: bold"">Lecture 7 Part 1/2: Derivatives of Random Functions (fundamentally just the reparameterization trick)</p>

<hr>

<p style="color: red; font-weight: bold"">Lecture 7 Part 2/2: Second Derivatives, Bilinear Forms, and Hessian Matrices [<a href="./matrix_calculus/6_2_calculus_of_variations_and_gradients_of_functionals.pdf">PDF</a>]</p>

<hr>

<p style="color: gray; font-weight: bold"">Lecture 8 Part 1/2: Derivatives of Eigenproblems (lecture was a bit too brief) </p>

<hr>

<p style="color: gray; font-weight: bold"">Lecture 8 Part 2/2: Forward and Reverse-Mode Automatic Differentiation on Computational Graphs (I merged this with Lecture 5 Part 3/3) </p>


</html>