<html>

<!-- 
<style>
.content {
  max-width: 1000px;
  margin: auto;
  text-align: center;
}
.box {
  width: 1000px;
  padding: 10px;
  margin: auto;
  text-align: center;
}

</style>
 -->

<p><strong>My notes for <u>MIT 18.S096 Matrix Calculus For Machine Learning And Beyond</u></strong></p>
<p>To view a Jupyter notebook (ipynb), copy the link and paste it here https://nbviewer.org/.</p>

<i style="color: red">Lecture 4 Part 1: Gradients and Inner Products in Other Vector Space</i> [<a href="./matrix_calculus/4_1_gradient_and_inner_products_in_other_vector_spaces.pdf">PDF</a>]</li>
<ul>
<li>Riesz representation theorem</li>
<li>Gradient of Frobenius norm of A with respect to A</li>
<li>Gradient of x.T @ A @ y with respect to A</li>
<li>Gradient of sum(A) with respect to A</li>
</ul>
	
<i style="color: red">Lecture 5 Part 1: Derivative of Matrix Determinant and Inverse</i> [<a href="./matrix_calculus/5_1_derivative_of_matrix_determinant_and_inverse.pdf">PDF</a>]</li>
<ul>
<li>Norms and derivatives</li>
<li>Derivative of matrix determinant -- Jacobi's formula (d(det(A)) = ...)</li>
<li>Direct proof using Laplace expansion</li>
<li>Fancy proof using the similar behavior of determinant and trace near identity</li>
<li>Applications (Newton's method for finding eigenvalues, log determinant)</li>
<li>Derivative of matrix inverse</li>
</ul>

<i style="color: red">Lecture 5 Part 2: Forward-mode Automatic Differentiation using Dual Numbers</i> [<a href="./matrix_calculus/5_2_forward_mode_automatic_differentiation_using_dual_numbers.ipynb">ipynb</a>]</li>
<ul>
</ul>

<i style="color: red">Lecture 5 Part 3: Reverse-mode Automatic Differentiation</i> [<a href="./matrix_calculus/5_3_reverse_mode_automatic_differentiation.ipynb">ipynb</a>]</li>
<ul>
</ul>

</html>